{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03776b0",
   "metadata": {},
   "source": [
    "### 2. Define Prototype Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae72c8c",
   "metadata": {},
   "source": [
    "***\n",
    "#### PANTHER: Global Prototype Generation\n",
    "\n",
    "This section executes the **PANTHER** workflow to extract representative tissue patterns (prototypes) from the CPTAC dataset.\n",
    "\n",
    "##### 1. Configuration & Setup\n",
    "* **Source**: Appends PANTHER scripts to `sys.path` and configures CPTAC feature paths.\n",
    "* **Hyperparameters**: Generates **16 prototypes** (`n_proto`) using **K-means** clustering on a sample of **10,000 patches** (`n_proto_patches`) with a **1536-d** feature space.\n",
    "\n",
    "##### 2. Data Loading\n",
    "* **Dataset**: Uses `WSIProtoDataset` to load UNI features from LSCC and LUAD H5 files based on pre-defined `train` splits.\n",
    "\n",
    "##### 3. Execution & Saving\n",
    "* **`cluster()`**: Samples patches across the cohort and performs K-means to find 16 centroids (tissue patterns).\n",
    "* **Storage**: Saves the centroids as a `.pkl` file to be used as a reference for downstream analysis (e.g., local prototype counting or graph building)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deeb4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following split names: ['train']\n",
      "\n",
      "successfully read splits for:  ['train']\n",
      "split: train, n: 1662\n",
      "\n",
      "Init Datasets... Sampling maximum of 160000 patches: 97 each from 1662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1662 [00:00<?, ?it/s]/workspace/HDDX/Pathology_Graph/github/PANTHER/src/utils/proto_utils.py:48: UserWarning: you are shuffling a 'Tensor' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(data_reshaped)\n",
      "100%|██████████| 1662/1662 [05:33<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total of 160000 patches aggregated\n",
      "\n",
      "Using Kmeans for clustering...\n",
      "\n",
      "\tNum of clusters 16, num of iter 50\n",
      "\n",
      "Clustering took 289.72745847702026 seconds!\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '/workspace/HDDX/Pathology_Graph'\n",
    "split_n = 'split_0'\n",
    "\n",
    "import sys\n",
    "sys.path.append(f'{BASE_DIR}/github/PANTHER/src')\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from wsi_datasets import WSIProtoDataset\n",
    "from utils.utils import seed_torch, read_splits\n",
    "from utils.file_utils import save_pkl\n",
    "from utils.proto_utils import cluster\n",
    "\n",
    "import os\n",
    "from os.path import join as j_\n",
    "\n",
    "def build_datasets(csv_splits, batch_size=1, num_workers=2, train_kwargs={}):\n",
    "    dataset_splits = {}\n",
    "    for k in csv_splits.keys(): # ['train']\n",
    "        df = csv_splits[k]\n",
    "        dataset_kwargs = train_kwargs.copy()\n",
    "        dataset = WSIProtoDataset(df, **dataset_kwargs)\n",
    "\n",
    "        batch_size = 1\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "        dataset_splits[k] = dataloader\n",
    "        print(f'split: {k}, n: {len(dataset)}')\n",
    "\n",
    "    return dataset_splits\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=1)\n",
    "# model / loss fn args ###\n",
    "parser.add_argument('--n_proto', type=int, default=16)\n",
    "parser.add_argument('--n_proto_patches', type=int, default=10000)\n",
    "parser.add_argument('--n_init', type=int, default=5)\n",
    "parser.add_argument('--n_iter', type=int, default=50)\n",
    "parser.add_argument('--in_dim', type=int, default=1536)\n",
    "parser.add_argument('--mode', type=str, choices=['kmeans', 'faiss'], default='kmeans')\n",
    "\n",
    "# dataset / split args ###\n",
    "parser.add_argument('--data_source', type=str, default=None)\n",
    "parser.add_argument('--split_dir', type=str, default=f'./splits/CPTAC/{split_n}/')\n",
    "parser.add_argument('--split_names', type=str, default='train')\n",
    "parser.add_argument('--num_workers', type=int, default=0)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "args.data_source = [f'{BASE_DIR}/datasource/CPTAC/LSCC_CLAM/patch_512/uni_features/feats_h5',\n",
    "                    f'{BASE_DIR}/datasource/CPTAC/LUAD_CLAM/patch_512/uni_features/feats_h5']\n",
    "\n",
    "# data loading\n",
    "train_kwargs = dict(data_source=args.data_source)\n",
    "seed_torch(args.seed)\n",
    "csv_splits = read_splits(args)\n",
    "print('\\nsuccessfully read splits for: ', list(csv_splits.keys()))\n",
    "\n",
    "# run Panther prototyping\n",
    "dataset_splits = build_datasets(csv_splits,\n",
    "                                batch_size=1,\n",
    "                                num_workers=args.num_workers,\n",
    "                                train_kwargs=train_kwargs)\n",
    "print('\\nInit Datasets...', end=' ')\n",
    "\n",
    "\n",
    "os.makedirs(j_(args.split_dir, 'prototypes'), exist_ok=True)\n",
    "loader_train = dataset_splits['train']\n",
    "\n",
    "tmp = next(iter(loader_train))\n",
    "\n",
    "_, weights = cluster(loader_train,\n",
    "                     n_proto=args.n_proto,\n",
    "                     n_iter=args.n_iter,\n",
    "                     n_init=args.n_init,\n",
    "                     feature_dim=args.in_dim,\n",
    "                     mode=args.mode,\n",
    "                     n_proto_patches=args.n_proto_patches,\n",
    "                     use_cuda=True if torch.cuda.is_available() else False)\n",
    "\n",
    "\n",
    "save_fpath = j_(args.split_dir,\n",
    "                'prototypes',\n",
    "                f\"prototypes_c{args.n_proto}_{args.data_source[0].split('/')[-2]}_{args.mode}_num_{args.n_proto_patches:.1e}.pkl\")\n",
    "\n",
    "save_pkl(save_fpath, {'prototypes': weights})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66e0667",
   "metadata": {},
   "source": [
    "***\n",
    "#### Feature Quantization: Bag-of-Prototypes Encoding\n",
    "\n",
    "This script quantifies Whole Slide Images (WSIs) by mapping individual patch features to the previously generated global prototypes.\n",
    "\n",
    "##### 1. Resource Preparation\n",
    "* **Prototypes**: Loads the 16-dimensional centroids and ensures they are in `float32` format for GPU-accelerated computation.\n",
    "* **Target Data**: Targets `.h5` files containing patch-level embeddings (e.g., UNI features).\n",
    "\n",
    "##### 2. Similarity Mapping & Assignment\n",
    "* **Normalization**: Applies L2-normalization to both patch features and prototypes to calculate **cosine similarity** via matrix multiplication (`torch.mm`).\n",
    "* **Assignment**: Maps each patch to its most similar prototype using `torch.argmax`.\n",
    "\n",
    "##### 3. Counting & Serialization\n",
    "* **Aggregation**: Uses `torch.bincount` to calculate the frequency of each prototype (0–15) within the slide, creating a **\"Bag-of-Prototypes\"** vector.\n",
    "* **Output**: Saves the resulting 16-dimensional frequency tensor as a `.pt` file, which serves as a simplified explanatory representation (`expl`) of the slide's morphology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2162/2162 [02:05<00:00, 17.22it/s] \n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Settings\n",
    "BASE_DIR = '/workspace/HDDX/Pathology_Graph'\n",
    "split_n = 'split_0'\n",
    "NUM_PROTOTYPES = 16 \n",
    "\n",
    "H5_FILES = sorted(glob(f'{BASE_DIR}/datasource/CPTAC/*_CLAM/patch_512/uni_features/feats_h5/*.h5'))\n",
    "PROTO_PATH = f'./splits/CPTAC/{split_n}/prototypes/prototypes_c16_uni_features_kmeans_num_1.0e+04.pkl'\n",
    "SAVE_DIR = f'./splits/CPTAC/{split_n}/expl_16x16'\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1. Load prototypes\n",
    "with open(PROTO_PATH, 'rb') as f:\n",
    "    proto_data = pickle.load(f)\n",
    "proto_tensor = torch.from_numpy(proto_data['prototypes'].squeeze()).to(device).float()\n",
    "\n",
    "for h5_path in tqdm(H5_FILES):\n",
    "    slide_id = os.path.basename(h5_path).replace('.h5', '')\n",
    "    \n",
    "    try:\n",
    "        with h5py.File(h5_path, 'r') as h5:\n",
    "            feats = torch.from_numpy(h5['features'][:]).to(device).float()\n",
    "    \n",
    "        target_dtype = proto_tensor.dtype # float32\n",
    "        \n",
    "        norm_feats = torch.nn.functional.normalize(feats.to(target_dtype), dim=1)\n",
    "        norm_protos = torch.nn.functional.normalize(proto_tensor.to(target_dtype), dim=1)\n",
    "        \n",
    "        # calculate cosine similarity\n",
    "        sim = torch.mm(norm_feats, norm_protos.t()) \n",
    "        \n",
    "        # assign each patch to the most similar prototype\n",
    "        global_cluster_labels = torch.argmax(sim, dim=1)\n",
    "        count_expl = torch.bincount(global_cluster_labels, minlength=NUM_PROTOTYPES).float()\n",
    "        \n",
    "        torch.save(count_expl.cpu(), f'{SAVE_DIR}/{slide_id}_expl.pt')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {slide_id}: {e}\")\n",
    "        continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "251114_patho_env",
   "language": "python",
   "name": "251114_patho_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
